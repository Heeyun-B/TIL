# Prompt Engineering

컨텍스트 길이 : 한 번에 물어볼 수 있는 글자수의 크기

GPT와 계속 대화하다 보면 어느 순간 처음 물어본 질문에 대해서는 대답을 잘 못함 → 컨텍스트 길이때문.

## 토큰

- **정의**: GPT와 같은 인공지능 모델에서 텍스트 데이터를 처리하고 이해하는 기본 단위
(토큰은 문장에서 단어로 생각할 수 있음)
    - 각 토큰별로 고유한 숫자가 매겨져있음.
    - 영어보다 한글 문장을 표현하는데 더 많은 토큰 소요됨.
        - 이유: 한글은 다양한 조합형 문자로 인해 많은 토큰이 필요
- **최대 입력 토큰 제한**: 각 LLM 모델마다 최대로 입력할 수 있는 토큰 수가 제한됨.
- **토큰 수 확인**: GPT의 입력한 문장에 대한 토큰 수를 확인하기 위한 [사이트](https://platform.openai.com/tokenizer)

## API 주요 파라미터

- 필수 파라미터
    - model: 사용하고자dd하는 gpt 모델 + messages: 대화 메시지 목록 응답의 다양성 제어 파라미터
- temperature: 다음 토큰 예측을 위한 확률 분포를 부드럽게 하는 역할
    - 0에서 2 사이의 값을 가지며, 1.0 이상일 경우 확률 분포가 평탄해지며 더 창의적이고 예측할 수 없는 결과를 생성
- top_p: 누적 확률을 기준으로 선택할 토큰의 범위를 제한
    - 0에서 1 사이의 값을 가지며, 1에 가까울수록 모델은 더 다양한 토큰을 고려
    
    낮은 temperature + 높은 top_p
    
    - temperature가 낮아 확률 분포의 차이가 더 강조됨 → 높은 확률의 단어에 집중
    - top_p가 높아 누적 확률 범위가 넓음 → 상위 확률 단어들을 다양하게 고려
        - 응답이 안정적이고 예측 가능
    - 활용: 기술 문서 작성, 고객 지원
    
    높은 temperature + 낮은 top_p
    
    - temperature가 높아 확률 분포가 평탄해짐 → 다양한 단어가 선택될 가능성 증가
    - top_p가 낮아 누적 확률 범위가 좁음 → 선택 후보가 제한적
        - 창의적이고 독창적인 응답
    - 활용: 아이디어 도출, 소설 생성

Single-turn 대화 : 한 번의 질문과 한 번의 응답으로 이루어진 대화 형태 (이전 대화 내용 기억 못함)  

이어서 대화화기 [Multi-turn 대화]
- GPT가 이전 대화를 기억함으로써 대화의 맥락을 파악하여 답변.
- 이전 대화를 기억하기 위해서 어떠한 원리가 필요할까?
- 멀티턴 대화 실습
    - 아래 실습 코드는 과학적 질문을 답변하는 페르소나임
    - GPT에게 게임 캐릭터와 같이 역할 및 말투 등의 스타일을 지정
    - 대화 내용을 기억하면서 답변하는 것을 볼 수 있음

### 명시적 지시

- 프롬프트 구조화, 질문에 대한 배경 정보 제공 및 명확한 지시는 더 나은 결과를 얻습니다.
    - 프롬프트 형식 지정, 맥락 제공, 페르소나 지정, 제한 사항 지정, 출력 형식 지정 등

### 퓨샷 프롬프트(Few-Shot Prompt)

- 프롬프트에 몇 가지 예시를 제공하여 모델이 특정한 방식으로 답변을 생성하도록 유도하는 방법

### 생각의 사슬(Chain-of-Thought)
- **설명**:
    - 최종 답을 바로 도출하는 것이 아니라, 문제를 해결하기 위한 중간 단계를 거쳐 사고 과정을 설명함으로써 더 정확하고 일관된 답을 도출.
    - 단순히 "단계적으로 문제를 해결하세요.(Let's think step by step)" 라는 프롬프트를 넣어도 생각의 사슬 효과를 어느정도 얻을 수 있음.
    - 답변한 것을 토대로 "위의 답변이 올바른지 평가해보세요" 라는 프롬프트를 넣으면 평가하는 과정도 생성되어 올바른 결과를 답변할 가능성이 높아짐.
- **원리**: GPT에게 결과에 대한 근거를 입력으로 줌으로써 올바른 결과 도출 유도함
- **활용**: 계산 문제나 논리적 추론을 요하는 문제에서 유용.

### 자기 일관성 (Self-Consistency)

- 거대 언어 모델(LLM)은 확률론적 성격을 지니고 있어, Chain-of-Thought와 같은 기법을 사용하더라도 단일 생성으로는 종종 잘못된 결과가 나올 수 있음.
- 자기 일관성(Self-Consistency)은 다수의 생성을 통해 가장 빈번하게 나타나는 답을 선택함으로써 정확성을 높이는 방법.
- 단점: 더 높은 계산 비용을 수반.

### 생각의 나무, 사고의 구조도 (Tree-of-Thought)

- **설명**:
    - 생각의 사슬(Chain-of-Thought) 기법을 확장한 방식으로, 여러 가지 사고 경로를 동시에 탐색하는 방법.
    - 문제 해결 과정을 Tree 구조로 표현하여 다양한 가능성을 고려하고 최적의 해결책을 찾아나감.
    - 인공지능이 성급하게 결정하지 않고,여러가지 가능성을 먼저 생각한 뒤 가장 가능성이 높은 것을 선택하면서 점차 발전시키는 기술
- **원리**:
    - 복잡한 문제를 작은 하위 문제로 분해하고, 각 하위 문제에 대해 여러 해결 방안을 탐색함.
    - 확률적 추론을 통해 가장 가능성 있는 해결 경로를 선택하여 최종 답안에 도달함.
- **CoT 와 비교**
    - CoT:
        - 선형적인 사고 과정을 따르며 문제 해결을 위한 단계들이 순차적으로 연결되어 있음.
        - 비교적 단순하거나 선형적인 추론이 필요한 문제에 적합.
    - ToT:
        - 여러 가능한 경로를 동시에 탐색할 수 있어 더 복잡한 문제 해결이 가능.
        - 전략적 사고, 복잡한 의사결정, 창의적 문제 해결 등 더 넓은 범위의 문제에 적용 가능.

### 인터넷 검색 기반 응답

- 외부 지식을 토대로 신뢰성 있는 답변 받기
- 실습
    1. 위키피디아의 정보를 잘 가지고 오는지 `page_title`의 변수 값을 바꿔가보면서 출력되는 내용을 확인한다.
    2. GPT에게 질문할 내용을 위키피디아의 검색어로 사용할 수 있도록 GPT를 통해 질문의 키워드를 추출한다.
    3. 완성된 실습 코드에서 아래와 같은 예시 프롬프트로 실습해본다.
        - 위키피디아에 있는 정보 : 마이크로소프트의 최신 기술에 대해 알려줘.
        - 위키피디아에 없는 정보 : 사과 회사에 대한 핵심 기술에 대해 알려줘